{\color{gray}\hrule}
\begin{center}
\section{System Architecture}
\bigskip
\end{center}
{\color{gray}\hrule}

\subsection{Base Architecture}

\subsubsection{Summary}
The Base Architecture is what is implemented in both RAG and KRAG architectures. The Base Architecture uses the dotenv module to get a user's API key from a .env file, loads up the embedding with the API key, loads up a vectorstorage variable for similarity search, and loads up large language model (LLM) using the API key. Both RAG and KRAG architectures also have token control functionality as OpenAI models have token limits to control bandwidth.

\subsubsection{Similarity Search}
Similarity search implements the "retrieval" part of "retrieval augment generate". The system retrieves OpenAI's embeddings and Facebook AI Similarity Search (FAISS) using the langchain module. The system passes the processed chunks of \textit{A Song of Ice and Fire} books and OpenAI's embedding into the FAISS object and retrieves the k most similar chunks using its similarity\_search method.

\subsubsection{Token Control}
OpenAI's gpt-3.5-turbo model has an total token limit of 4096 tokens and reserves 256 tokens for generating a response. This gives the system 3840 tokens for a query. Both RAG and KRAG implementations use the tiktoken module to make sure queries do not go over limit.

\subsection{RAG}
The RAG architecture passes two parameters to an LLM's query: a question and a context. First, a user enters a question into the system. As mentioned earlier, the "retrieval" in "retrieval augment generate" is done using FAISS. FAISS finds the k-nearest chunks or files of a given question, which in this implementation is the 3 nearest 1,000 character chunks of the books; this is the "context" of the query. Since a user's question cannot be cut from a query to get a proper response from an LLM, the system does a calculation to see how many tokens need to be cut from the context in order to meet the token requirements. Once the context tokens are cut appropriately, the context and question are passed to a template that serve as the query passed to the LLM for a response. The context and question stitched to a template for query act as the "augment" portion of "retrieval augment generate" and the response from the LLM acts as the "generate" portion.

\subsection{KRAG}
The KRAG implementation is nearly identical to the RAG implementation, but it includes an addition query item, knowledge graphs triples, and appropriate token controls for the additional item. Given a question by a user, the KRAG query, on top of what the RAG query already does, finds the entities in the given question that match those in the knowledge graph, finds the relationships that entity has with other entities/nodes, and returns that information in the form of triples, a data structure developed by Tim Berners-Lee. After some token control, those triples are passed to the query along with context and question, which is passed to the LLM to generate a response.

\bigskip